{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ruta_data = 'DatasetFuegoHumo/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:\n",
      "Total de imágenes analizadas: 300\n",
      "Imágenes en escala de grises (o no RGB): 0\n",
      "Porcentaje en escala de grises: 0.00%\n",
      "\n",
      "Val:\n",
      "Total de imágenes analizadas: 3096\n",
      "Imágenes en escala de grises (o no RGB): 0\n",
      "Porcentaje en escala de grises: 0.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ruta_train_imagenes = os.path.join(ruta_data, \"train\", \"images\")\n",
    "ruta_val_imagenes = os.path.join(ruta_data, \"val\", \"images\")\n",
    "\n",
    "\n",
    "def contar_escala_grises(ruta_imagenes, nombre_carpeta, max_imagenes=None):\n",
    "    total_imagenes = 0\n",
    "    grises = 0\n",
    "    \n",
    "    lista_imagenes = [f for f in os.listdir(ruta_imagenes) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "    if max_imagenes:\n",
    "        lista_imagenes = lista_imagenes[:max_imagenes]\n",
    "    \n",
    "    for nombre_imagen in lista_imagenes:\n",
    "        ruta_imagen = os.path.join(ruta_imagenes, nombre_imagen)\n",
    "        imagen = Image.open(ruta_imagen)\n",
    "        \n",
    "    \n",
    "        if imagen.mode != 'RGB':\n",
    "            grises += 1\n",
    "            print(f\"{nombre_carpeta} - Imagen en escala de grises (o no RGB): {nombre_imagen} - Modo: {imagen.mode}\")\n",
    "        \n",
    "        total_imagenes += 1\n",
    "    \n",
    "    print(f\"\\n{nombre_carpeta}:\")\n",
    "    print(f\"Total de imágenes analizadas: {total_imagenes}\")\n",
    "    print(f\"Imágenes en escala de grises (o no RGB): {grises}\")\n",
    "    print(f\"Porcentaje en escala de grises: {(grises / total_imagenes * 100):.2f}%\")\n",
    "\n",
    "contar_escala_grises(ruta_train_imagenes, \"Train\", max_imagenes=300) \n",
    "contar_escala_grises(ruta_val_imagenes, \"Val\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def cargar_datos(ruta_imagenes, ruta_labels, max_imagenes=None):\n",
    "    imagenes = []\n",
    "    etiquetas = []\n",
    "    \n",
    "    lista_imagenes = [f for f in os.listdir(ruta_imagenes) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "    if max_imagenes:\n",
    "        lista_imagenes = lista_imagenes[:max_imagenes]\n",
    "    \n",
    "    for nombre_imagen in lista_imagenes:\n",
    "        \n",
    "        ruta_imagen = os.path.join(ruta_imagenes, nombre_imagen)\n",
    "        imagen = Image.open(ruta_imagen).resize((128, 128))\n",
    "        imagen = np.array(imagen) / 255.0  # Normaliza a [0, 1]\n",
    "        \n",
    "        # Etiqueta basada en .txt (solo para entrenamiento)\n",
    "        nombre_base = os.path.splitext(nombre_imagen)[0]\n",
    "        ruta_txt = os.path.join(ruta_labels, f\"{nombre_base}.txt\")\n",
    "        if os.path.exists(ruta_txt):\n",
    "            with open(ruta_txt, 'r', encoding='utf-8') as archivo:\n",
    "                contenido = archivo.read().strip()\n",
    "                etiqueta = 1 if contenido else 0  # 1 si hay fuego/humo, 0 si no\n",
    "        else:\n",
    "            etiqueta = 0\n",
    "        \n",
    "        imagenes.append(imagen)\n",
    "        etiquetas.append(etiqueta)\n",
    "    \n",
    "    return np.array(imagenes), np.array(etiquetas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga datos de entrenamiento y validación\n",
    "ruta_train_imagenes = os.path.join(ruta_data, \"train\", \"images\")\n",
    "ruta_train_labels = os.path.join(ruta_data, \"train\", \"labels\")\n",
    "ruta_val_imagenes = os.path.join(ruta_data, \"val\", \"images\")\n",
    "ruta_val_labels = os.path.join(ruta_data, \"val\", \"labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = cargar_datos(ruta_train_imagenes, ruta_train_labels, max_imagenes=900)\n",
    "X_val, y_val = cargar_datos(ruta_val_imagenes, ruta_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Quimey\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Define el modelo CNN \n",
    "modelo = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),  # Más filtros\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),  # Capa adicional\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),           \n",
    "    layers.Dropout(0.5),                            # Evita sobreajuste\n",
    "    layers.Dense(1, activation='sigmoid')           # Salida binaria\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modelo.compile(optimizer='adam',\n",
    "               loss='binary_crossentropy',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 5s/step - accuracy: 0.9062 - loss: 0.2069 - val_accuracy: 0.4438 - val_loss: 10.7331\n",
      "Epoch 2/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 2s/step - accuracy: 0.9961 - loss: 0.0424 - val_accuracy: 0.4438 - val_loss: 4.9285\n",
      "Epoch 3/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 1s/step - accuracy: 0.9948 - loss: 0.0514 - val_accuracy: 0.4438 - val_loss: 5.5852\n",
      "Epoch 4/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - accuracy: 0.9953 - loss: 0.0374 - val_accuracy: 0.4438 - val_loss: 3.6862\n",
      "Epoch 5/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step - accuracy: 0.9972 - loss: 0.0260 - val_accuracy: 0.4438 - val_loss: 4.3115\n",
      "Epoch 6/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.9944 - loss: 0.0419 - val_accuracy: 0.4438 - val_loss: 4.7563\n",
      "Epoch 7/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1s/step - accuracy: 0.9968 - loss: 0.0264 - val_accuracy: 0.4438 - val_loss: 3.3855\n",
      "Epoch 8/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 1s/step - accuracy: 0.9937 - loss: 0.0413 - val_accuracy: 0.4438 - val_loss: 3.7313\n",
      "Epoch 9/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2s/step - accuracy: 0.9977 - loss: 0.0226 - val_accuracy: 0.4438 - val_loss: 3.2457\n",
      "Epoch 10/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 1s/step - accuracy: 0.9938 - loss: 0.0359 - val_accuracy: 0.4438 - val_loss: 4.4612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x229858e5780>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "modelo.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados en validación:\n",
      "Precisión (Accuracy): 44.38%\n"
     ]
    }
   ],
   "source": [
    "accuracy = modelo.evaluate(X_val, y_val, verbose=0)[1]\n",
    "accuracy_porcentaje = accuracy * 100\n",
    "\n",
    "print(\"\\nResultados en validación:\")\n",
    "print(f\"Precisión (Accuracy): {accuracy_porcentaje:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:\n",
      "Imágenes con fuego/humo (1): 2\n",
      "Imágenes sin fuego/humo (0): 298\n",
      "Total de imágenes: 300\n",
      "Porcentaje de fuego/humo: 0.67%\n",
      "Porcentaje sin fuego/humo: 99.33%\n",
      "\n",
      "Val:\n",
      "Imágenes con fuego/humo (1): 1722\n",
      "Imágenes sin fuego/humo (0): 1374\n",
      "Total de imágenes: 3096\n",
      "Porcentaje de fuego/humo: 55.62%\n",
      "Porcentaje sin fuego/humo: 44.38%\n"
     ]
    }
   ],
   "source": [
    "# Función para contar etiquetas\n",
    "def contar_etiquetas(ruta_imagenes, ruta_labels, nombre_carpeta, max_imagenes=None):\n",
    "    positivos = 0  # Fuego/humo\n",
    "    negativos = 0  # Sin fuego/humo\n",
    "    \n",
    "    # Lista de imágenes\n",
    "    lista_imagenes = [f for f in os.listdir(ruta_imagenes) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "    if max_imagenes:\n",
    "        lista_imagenes = lista_imagenes[:max_imagenes]\n",
    "    \n",
    "    total_imagenes = len(lista_imagenes)\n",
    "    \n",
    "    for nombre_imagen in lista_imagenes:\n",
    "        nombre_base = os.path.splitext(nombre_imagen)[0]\n",
    "        ruta_txt = os.path.join(ruta_labels, f\"{nombre_base}.txt\")\n",
    "        \n",
    "        if os.path.exists(ruta_txt):\n",
    "            with open(ruta_txt, 'r', encoding='utf-8') as archivo:\n",
    "                contenido = archivo.read().strip()\n",
    "                if contenido:  # Si hay coordenadas\n",
    "                    positivos += 1\n",
    "                else:  # Si está vacío\n",
    "                    negativos += 1\n",
    "        else:  # Si no hay .txt\n",
    "            negativos += 1\n",
    "    print(f\"\\n{nombre_carpeta}:\")\n",
    "    print(f\"Imágenes con fuego/humo (1): {positivos}\")\n",
    "    print(f\"Imágenes sin fuego/humo (0): {negativos}\")\n",
    "    print(f\"Total de imágenes: {total_imagenes}\")\n",
    "    print(f\"Porcentaje de fuego/humo: {(positivos / total_imagenes * 100):.2f}%\")\n",
    "    print(f\"Porcentaje sin fuego/humo: {(negativos / total_imagenes * 100):.2f}%\")\n",
    "\n",
    "    # Conta en train y val\n",
    "contar_etiquetas(ruta_train_imagenes, ruta_train_labels, \"Train\", max_imagenes=300)\n",
    "contar_etiquetas(ruta_val_imagenes, ruta_val_labels, \"Val\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_train_imagenes = os.path.join(ruta_data, \"train\", \"images\")\n",
    "ruta_train_labels = os.path.join(ruta_data, \"train\", \"labels\")\n",
    "ruta_val_imagenes = os.path.join(ruta_data, \"val\", \"images\")\n",
    "ruta_val_labels = os.path.join(ruta_data, \"val\", \"labels\")\n",
    "\n",
    "# Sin límite en train para usar todas las imágenes\n",
    "X_train, y_train = cargar_datos(ruta_train_imagenes, ruta_train_labels, max_imagenes=1200)\n",
    "X_val, y_val = cargar_datos(ruta_val_imagenes, ruta_val_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Positivos: 5, Negativos: 1195\n",
      "Peso para clase positiva: 239.00\n"
     ]
    }
   ],
   "source": [
    "# Calcula pesos de clase para balancear\n",
    "num_positivos = np.sum(y_train == 1)\n",
    "num_negativos = np.sum(y_train == 0)\n",
    "peso_positivo = num_negativos / num_positivos if num_positivos > 0 else 1\n",
    "class_weights = {0: 1.0, 1: peso_positivo}\n",
    "\n",
    "print(f\"Train - Positivos: {num_positivos}, Negativos: {num_negativos}\")\n",
    "print(f\"Peso para clase positiva: {peso_positivo:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "modelo = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modelo.compile(optimizer='adam',\n",
    "               loss='binary_crossentropy',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 4s/step - accuracy: 0.9210 - loss: 8.0286 - val_accuracy: 0.4438 - val_loss: 2.8329\n",
      "Epoch 2/3\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - accuracy: 0.6872 - loss: 3.1118 - val_accuracy: 0.4648 - val_loss: 1.6810\n",
      "Epoch 3/3\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - accuracy: 0.6223 - loss: 1.2221 - val_accuracy: 0.4302 - val_loss: 2.9581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x229a05a6a40>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrena el modelo con pesos de clase\n",
    "modelo.fit(X_train, y_train, epochs=3, batch_size=32, \n",
    "           validation_data=(X_val, y_val), class_weight=class_weights, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados en validación:\n",
      "Precisión (Accuracy): 43.02%\n"
     ]
    }
   ],
   "source": [
    "# Calcula la accuracy\n",
    "accuracy = modelo.evaluate(X_val, y_val, verbose=0)[1]\n",
    "accuracy_porcentaje = accuracy * 100\n",
    "\n",
    "print(\"\\nResultados en validación:\")\n",
    "print(f\"Precisión (Accuracy): {accuracy_porcentaje:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Positivos originales: 5, Negativos: 1195\n"
     ]
    }
   ],
   "source": [
    "# Separa imágenes positivas y negativas\n",
    "positivas_idx = np.where(y_train == 1)[0]\n",
    "negativas_idx = np.where(y_train == 0)[0]\n",
    "X_positivas = X_train[positivas_idx]\n",
    "X_negativas = X_train[negativas_idx]\n",
    "\n",
    "print(f\"Train - Positivos originales: {len(X_positivas)}, Negativos: {len(X_negativas)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aumentación para imágenes positivas\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Positivos después de aumentación: 259, Negativos: 1195\n"
     ]
    }
   ],
   "source": [
    "# Genera más imágenes positivas hasta igualar (o acercarse) a las negativas\n",
    "X_positivas_aumentadas = []\n",
    "y_positivas_aumentadas = []\n",
    "target_positivas = min(len(X_negativas), len(X_positivas) * 50)  # Límite razonable\n",
    "for i in range(len(X_positivas)):\n",
    "    img = X_positivas[i]\n",
    "    img = img.reshape((1,) + img.shape)  # Añade dimensión batch\n",
    "    for batch in datagen.flow(img, batch_size=1):\n",
    "        X_positivas_aumentadas.append(batch[0])\n",
    "        y_positivas_aumentadas.append(1)\n",
    "        if len(X_positivas_aumentadas) >= target_positivas:\n",
    "            break\n",
    "\n",
    "# Combina datos originales y aumentados\n",
    "X_train_balanced = np.concatenate([X_negativas, X_positivas, np.array(X_positivas_aumentadas)])\n",
    "y_train_balanced = np.concatenate([np.zeros(len(X_negativas)), np.ones(len(X_positivas)), np.ones(len(X_positivas_aumentadas))])\n",
    "\n",
    "print(f\"Train - Positivos después de aumentación: {len(y_train_balanced[y_train_balanced == 1])}, Negativos: {len(y_train_balanced[y_train_balanced == 0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo con MobileNetV2\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "modelo = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compila el modelo\n",
    "modelo.compile(optimizer='adam',\n",
    "               loss='binary_crossentropy',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 3s/step - accuracy: 0.8504 - loss: 0.4105 - val_accuracy: 0.4612 - val_loss: 2.6000\n",
      "Epoch 2/2\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step - accuracy: 0.9963 - loss: 0.0270 - val_accuracy: 0.4968 - val_loss: 1.7221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x229a48cd8d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrena el modelo\n",
    "modelo.fit(X_train_balanced, y_train_balanced, epochs=2, batch_size=32, validation_data=(X_val, y_val), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados en validación:\n",
      "Precisión (Accuracy): 49.68%\n"
     ]
    }
   ],
   "source": [
    "# Calcula la accuracy\n",
    "accuracy = modelo.evaluate(X_val, y_val, verbose=0)[1]\n",
    "accuracy_porcentaje = accuracy * 100\n",
    "\n",
    "print(\"\\nResultados en validación:\")\n",
    "print(f\"Precisión (Accuracy): {accuracy_porcentaje:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Positivos originales: 5, Negativos originales: 1195\n"
     ]
    }
   ],
   "source": [
    "# Separa imágenes positivas y negativas\n",
    "positivas_idx = np.where(y_train == 1)[0]\n",
    "negativas_idx = np.where(y_train == 0)[0]\n",
    "X_positivas = X_train[positivas_idx]\n",
    "X_negativas = X_train[negativas_idx]\n",
    "\n",
    "print(f\"Train - Positivos originales: {len(X_positivas)}, Negativos originales: {len(X_negativas)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toma solo 400 negativos \n",
    "np.random.seed(42)  \n",
    "negativas_seleccionadas_idx = np.random.choice(negativas_idx, size=400, replace=False)\n",
    "X_negativas_seleccionadas = X_train[negativas_seleccionadas_idx]\n",
    "# Aumentación para llegar a 300 positivos\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "X_positivas_aumentadas = []\n",
    "y_positivas_aumentadas = []\n",
    "target_positivas = 300  \n",
    "for i in range(len(X_positivas)):\n",
    "    img = X_positivas[i]\n",
    "    img = img.reshape((1,) + img.shape)\n",
    "    count = 0\n",
    "    for batch in datagen.flow(img, batch_size=1):\n",
    "        X_positivas_aumentadas.append(batch[0])\n",
    "        y_positivas_aumentadas.append(1)\n",
    "        count += 1\n",
    "        if len(X_positivas_aumentadas) >= target_positivas - len(X_positivas):  # Restamos los originales\n",
    "            break\n",
    "    while len(X_positivas_aumentadas) < target_positivas - len(X_positivas) and count < 100:\n",
    "        for batch in datagen.flow(img, batch_size=1):\n",
    "            X_positivas_aumentadas.append(batch[0])\n",
    "            y_positivas_aumentadas.append(1)\n",
    "            count += 1\n",
    "            if len(X_positivas_aumentadas) >= target_positivas - len(X_positivas):\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Positivos después de aumentación: 304, Negativos: 400\n"
     ]
    }
   ],
   "source": [
    "# Combina datos: 400 negativos + 4 originales + 299 aumentados = 700 total\n",
    "X_train_balanced = np.concatenate([X_negativas_seleccionadas, X_positivas, np.array(X_positivas_aumentadas)])\n",
    "y_train_balanced = np.concatenate([np.zeros(400), np.ones(len(X_positivas)), np.ones(len(X_positivas_aumentadas))])\n",
    "\n",
    "print(f\"Train - Positivos después de aumentación: {len(y_train_balanced[y_train_balanced == 1])}, Negativos: {len(y_train_balanced[y_train_balanced == 0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo con MobileNetV2\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "modelo = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compila el modelo\n",
    "modelo.compile(optimizer='adam',\n",
    "               loss='binary_crossentropy',\n",
    "               metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 5s/step - accuracy: 0.9272 - loss: 0.1885 - val_accuracy: 0.4451 - val_loss: 4.4144\n",
      "Epoch 2/2\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 2s/step - accuracy: 0.9926 - loss: 0.0516 - val_accuracy: 0.4499 - val_loss: 3.1368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x229b00ea8c0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrena el modelo\n",
    "modelo.fit(X_train_balanced, y_train_balanced, epochs=2, batch_size=32, validation_data=(X_val, y_val), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 471ms/step\n",
      "\n",
      "Resultados en validación:\n",
      "Precisión (Accuracy): 44.99%\n",
      "Precision: 0.7714\n",
      "Recall: 0.0157\n",
      "F1-Score: 0.0307\n"
     ]
    }
   ],
   "source": [
    "# Calcula métricas\n",
    "predicciones = (modelo.predict(X_val) > 0.5).astype(int).flatten()\n",
    "accuracy = modelo.evaluate(X_val, y_val, verbose=0)[1]\n",
    "accuracy_porcentaje = accuracy * 100\n",
    "precision = precision_score(y_val, predicciones)\n",
    "recall = recall_score(y_val, predicciones)\n",
    "f1 = f1_score(y_val, predicciones)\n",
    "\n",
    "print(\"\\nResultados en validación:\")\n",
    "print(f\"Precisión (Accuracy): {accuracy_porcentaje:.2f}%\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Positivos originales: 5, Negativos originales: 1195\n"
     ]
    }
   ],
   "source": [
    " #Separa imágenes positivas y negativas\n",
    "positivas_idx = np.where(y_train == 1)[0]\n",
    "negativas_idx = np.where(y_train == 0)[0]\n",
    "X_positivas = X_train[positivas_idx]\n",
    "X_negativas = X_train[negativas_idx]\n",
    "\n",
    "print(f\"Train - Positivos originales: {len(X_positivas)}, Negativos originales: {len(X_negativas)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toma solo 200 negativos\n",
    "np.random.seed(42)\n",
    "negativas_seleccionadas_idx = np.random.choice(negativas_idx, size=200, replace=False)\n",
    "X_negativas_seleccionadas = X_train[negativas_seleccionadas_idx]\n",
    "# Aumentación para llegar a 300 positivos\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.4,\n",
    "    height_shift_range=0.4,\n",
    "    shear_range=0.4,\n",
    "    zoom_range=0.4,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.7, 1.3],\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Positivos después de aumentación: 304, Negativos: 200\n",
      "Forma de X_train_balanced: (504, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "X_positivas_aumentadas = []\n",
    "y_positivas_aumentadas = []\n",
    "target_positivas = 300\n",
    "for i in range(len(X_positivas)):\n",
    "    img = X_positivas[i]\n",
    "    img = img.reshape((1,) + img.shape)\n",
    "    count = 0\n",
    "    for batch in datagen.flow(img, batch_size=1):\n",
    "        X_positivas_aumentadas.append(batch[0])\n",
    "        y_positivas_aumentadas.append(1)\n",
    "        count += 1\n",
    "        if len(X_positivas_aumentadas) >= target_positivas - len(X_positivas):\n",
    "            break\n",
    "    while len(X_positivas_aumentadas) < target_positivas - len(X_positivas) and count < 100:\n",
    "        for batch in datagen.flow(img, batch_size=1):\n",
    "            X_positivas_aumentadas.append(batch[0])\n",
    "            y_positivas_aumentadas.append(1)\n",
    "            count += 1\n",
    "            if len(X_positivas_aumentadas) >= target_positivas - len(X_positivas):\n",
    "                break\n",
    "# Combina datos\n",
    "X_train_balanced = np.concatenate([X_negativas_seleccionadas, X_positivas, np.array(X_positivas_aumentadas)])\n",
    "y_train_balanced = np.concatenate([np.zeros(200), np.ones(len(X_positivas)), np.ones(len(X_positivas_aumentadas))])\n",
    "\n",
    "print(f\"Train - Positivos después de aumentación: {len(y_train_balanced[y_train_balanced == 1])}, Negativos: {len(y_train_balanced[y_train_balanced == 0])}\")\n",
    "print(f\"Forma de X_train_balanced: {X_train_balanced.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo con MobileNetV2\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-50]:\n",
    "    layer.trainable = False\n",
    "\n",
    "modelo = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compila el modelo\n",
    "modelo.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "               loss='binary_crossentropy',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 6s/step - accuracy: 0.8293 - loss: 0.3788 - val_accuracy: 0.6137 - val_loss: 0.7568\n",
      "Epoch 2/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - accuracy: 0.9914 - loss: 0.0360 - val_accuracy: 0.6214 - val_loss: 0.8030\n",
      "Epoch 3/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 3s/step - accuracy: 0.9983 - loss: 0.0112 - val_accuracy: 0.6269 - val_loss: 0.7815\n",
      "Epoch 4/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - accuracy: 0.9953 - loss: 0.0122 - val_accuracy: 0.6289 - val_loss: 0.7647\n",
      "Epoch 5/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 3s/step - accuracy: 0.9998 - loss: 0.0075 - val_accuracy: 0.6308 - val_loss: 0.7631\n",
      "Epoch 6/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 3s/step - accuracy: 0.9987 - loss: 0.0073 - val_accuracy: 0.6305 - val_loss: 0.7676\n",
      "Epoch 7/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 4s/step - accuracy: 0.9986 - loss: 0.0067 - val_accuracy: 0.6328 - val_loss: 0.7688\n",
      "Epoch 8/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 4s/step - accuracy: 0.9991 - loss: 0.0048 - val_accuracy: 0.6286 - val_loss: 0.7754\n",
      "Epoch 9/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3s/step - accuracy: 0.9989 - loss: 0.0041 - val_accuracy: 0.6311 - val_loss: 0.7778\n",
      "Epoch 10/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 3s/step - accuracy: 0.9975 - loss: 0.0057 - val_accuracy: 0.6337 - val_loss: 0.7687\n",
      "Epoch 11/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0032 - val_accuracy: 0.6379 - val_loss: 0.7761\n",
      "Epoch 12/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0032 - val_accuracy: 0.6305 - val_loss: 0.7923\n",
      "Epoch 13/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 7.5615e-04 - val_accuracy: 0.6282 - val_loss: 0.8075\n",
      "Epoch 14/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 3s/step - accuracy: 0.9984 - loss: 0.0048 - val_accuracy: 0.6266 - val_loss: 0.8299\n",
      "Epoch 15/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.6198 - val_loss: 0.8497\n",
      "Epoch 16/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3s/step - accuracy: 0.9994 - loss: 0.0076 - val_accuracy: 0.6189 - val_loss: 0.8687\n",
      "Epoch 17/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3s/step - accuracy: 0.9992 - loss: 0.0032 - val_accuracy: 0.6121 - val_loss: 0.8857\n",
      "Epoch 18/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 3s/step - accuracy: 0.9992 - loss: 0.0025 - val_accuracy: 0.6108 - val_loss: 0.8992\n",
      "Epoch 19/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3s/step - accuracy: 0.9955 - loss: 0.0068 - val_accuracy: 0.6150 - val_loss: 0.9158\n",
      "Epoch 20/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.6105 - val_loss: 0.9413\n",
      "Epoch 21/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step - accuracy: 0.9970 - loss: 0.0030 - val_accuracy: 0.6040 - val_loss: 0.9622\n",
      "Epoch 22/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - accuracy: 0.9918 - loss: 0.0234 - val_accuracy: 0.5927 - val_loss: 1.0310\n",
      "Epoch 23/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.5869 - val_loss: 1.0770\n",
      "Epoch 24/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 3s/step - accuracy: 0.9987 - loss: 0.0051 - val_accuracy: 0.5837 - val_loss: 1.0885\n",
      "Epoch 25/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 3s/step - accuracy: 0.9994 - loss: 0.0016 - val_accuracy: 0.5807 - val_loss: 1.1215\n",
      "Epoch 26/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 3s/step - accuracy: 0.9989 - loss: 0.0011 - val_accuracy: 0.5746 - val_loss: 1.1670\n",
      "Epoch 27/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - accuracy: 0.9994 - loss: 0.0016 - val_accuracy: 0.5711 - val_loss: 1.2237\n",
      "Epoch 28/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 8.8324e-04 - val_accuracy: 0.5669 - val_loss: 1.2894\n",
      "Epoch 29/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3s/step - accuracy: 0.9996 - loss: 0.0015 - val_accuracy: 0.5643 - val_loss: 1.3196\n",
      "Epoch 30/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 3s/step - accuracy: 0.9982 - loss: 0.0030 - val_accuracy: 0.5652 - val_loss: 1.2726\n"
     ]
    }
   ],
   "source": [
    "# Entrena el modelo\n",
    "history = modelo.fit(X_train_balanced, y_train_balanced, epochs=30, batch_size=32, validation_data=(X_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 413ms/step\n",
      "\n",
      "Resultados con umbral 0.5:\n",
      "Accuracy: 56.52%\n",
      "Precision: 0.7717\n",
      "Recall: 0.3101\n",
      "F1-Score: 0.4424\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 404ms/step\n",
      "\n",
      "Resultados con umbral 0.3:\n",
      "Accuracy: 59.33%\n",
      "Precision: 0.7365\n",
      "Recall: 0.4187\n",
      "F1-Score: 0.5339\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 401ms/step\n",
      "\n",
      "Resultados con umbral 0.2:\n",
      "Accuracy: 61.98%\n",
      "Precision: 0.7239\n",
      "Recall: 0.5116\n",
      "F1-Score: 0.5995\n"
     ]
    }
   ],
   "source": [
    "# Calcula métricas con umbral ajustable\n",
    "for umbral in [0.5, 0.3, 0.2]:\n",
    "    predicciones = (modelo.predict(X_val) > umbral).astype(int).flatten()\n",
    "    accuracy = np.mean(predicciones == y_val) * 100\n",
    "    precision = precision_score(y_val, predicciones)\n",
    "    recall = recall_score(y_val, predicciones)\n",
    "    f1 = f1_score(y_val, predicciones)\n",
    "    \n",
    "    print(f\"\\nResultados con umbral {umbral}:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo con MobileNetV2\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-50]:\n",
    "    layer.trainable = False\n",
    "\n",
    "modelo = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compila el modelo\n",
    "modelo.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "               loss='binary_crossentropy',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 5s/step - accuracy: 0.7697 - loss: 0.4069 - val_accuracy: 0.5155 - val_loss: 0.8841\n",
      "Epoch 2/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 3s/step - accuracy: 0.9836 - loss: 0.0511 - val_accuracy: 0.5320 - val_loss: 0.9218\n",
      "Epoch 3/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 3s/step - accuracy: 0.9962 - loss: 0.0191 - val_accuracy: 0.5304 - val_loss: 0.9065\n",
      "Epoch 4/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 3s/step - accuracy: 0.9873 - loss: 0.0228 - val_accuracy: 0.5313 - val_loss: 0.9071\n",
      "Epoch 5/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3s/step - accuracy: 0.9926 - loss: 0.0170 - val_accuracy: 0.5417 - val_loss: 0.8904\n",
      "Epoch 6/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 3s/step - accuracy: 0.9996 - loss: 0.0080 - val_accuracy: 0.5478 - val_loss: 0.8942\n",
      "Epoch 7/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 3s/step - accuracy: 0.9982 - loss: 0.0069 - val_accuracy: 0.5475 - val_loss: 0.8968\n",
      "Epoch 8/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 3s/step - accuracy: 0.9984 - loss: 0.0098 - val_accuracy: 0.5484 - val_loss: 0.9184\n",
      "Epoch 9/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 3s/step - accuracy: 0.9984 - loss: 0.0052 - val_accuracy: 0.5449 - val_loss: 0.9534\n",
      "Epoch 10/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0049 - val_accuracy: 0.5465 - val_loss: 0.9730\n",
      "Epoch 11/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.5439 - val_loss: 0.9942\n",
      "Epoch 12/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3s/step - accuracy: 0.9994 - loss: 0.0020 - val_accuracy: 0.5426 - val_loss: 1.0153\n",
      "Epoch 13/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 3s/step - accuracy: 0.9992 - loss: 0.0045 - val_accuracy: 0.5388 - val_loss: 1.0537\n",
      "Epoch 14/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 3s/step - accuracy: 0.9984 - loss: 0.0061 - val_accuracy: 0.5284 - val_loss: 1.1056\n",
      "Epoch 15/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 3s/step - accuracy: 0.9998 - loss: 0.0032 - val_accuracy: 0.5236 - val_loss: 1.1770\n",
      "Epoch 16/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - accuracy: 0.9982 - loss: 0.0053 - val_accuracy: 0.5216 - val_loss: 1.2153\n",
      "Epoch 17/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 3s/step - accuracy: 0.9978 - loss: 0.0054 - val_accuracy: 0.5171 - val_loss: 1.2442\n",
      "Epoch 18/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 3s/step - accuracy: 0.9991 - loss: 0.0029 - val_accuracy: 0.5145 - val_loss: 1.2891\n",
      "Epoch 19/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.5100 - val_loss: 1.3478\n",
      "Epoch 20/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 3s/step - accuracy: 0.9993 - loss: 0.0011 - val_accuracy: 0.5068 - val_loss: 1.4161\n",
      "Epoch 21/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.4919 - val_loss: 1.5970\n",
      "Epoch 22/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.4897 - val_loss: 1.6400\n",
      "Epoch 23/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.4871 - val_loss: 1.6895\n",
      "Epoch 24/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 4s/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.4790 - val_loss: 1.7654\n",
      "Epoch 25/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 9.7859e-04 - val_accuracy: 0.4748 - val_loss: 1.8425\n",
      "Epoch 26/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 9.1764e-04 - val_accuracy: 0.4716 - val_loss: 1.8993\n",
      "Epoch 27/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 7.0656e-04 - val_accuracy: 0.4703 - val_loss: 1.9366\n",
      "Epoch 28/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 7.6999e-04 - val_accuracy: 0.4745 - val_loss: 1.9674\n",
      "Epoch 29/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 7.2769e-04 - val_accuracy: 0.4748 - val_loss: 2.0122\n",
      "Epoch 30/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 5.6732e-04 - val_accuracy: 0.4713 - val_loss: 2.0750\n",
      "Epoch 31/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.4645 - val_loss: 2.1533\n",
      "Epoch 32/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 4s/step - accuracy: 1.0000 - loss: 4.2423e-04 - val_accuracy: 0.4603 - val_loss: 2.2372\n",
      "Epoch 33/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 3.8005e-04 - val_accuracy: 0.4622 - val_loss: 2.2617\n",
      "Epoch 34/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 5.7501e-04 - val_accuracy: 0.4616 - val_loss: 2.3076\n",
      "Epoch 35/35\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.4603 - val_loss: 2.3425\n"
     ]
    }
   ],
   "source": [
    "# Entrena el modelo\n",
    "history = modelo.fit(X_train_balanced, y_train_balanced, epochs=35, batch_size=32, validation_data=(X_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 429ms/step\n",
      "\n",
      "Resultados con umbral 0.5:\n",
      "Accuracy: 46.03%\n",
      "Precision: 0.7107\n",
      "Recall: 0.0499\n",
      "F1-Score: 0.0933\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 400ms/step\n",
      "\n",
      "Resultados con umbral 0.3:\n",
      "Accuracy: 49.16%\n",
      "Precision: 0.7587\n",
      "Recall: 0.1260\n",
      "F1-Score: 0.2161\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 404ms/step\n",
      "\n",
      "Resultados con umbral 0.2:\n",
      "Accuracy: 50.61%\n",
      "Precision: 0.7506\n",
      "Recall: 0.1678\n",
      "F1-Score: 0.2743\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 405ms/step\n",
      "\n",
      "Resultados con umbral 0.1:\n",
      "Accuracy: 53.17%\n",
      "Precision: 0.7329\n",
      "Recall: 0.2485\n",
      "F1-Score: 0.3712\n"
     ]
    }
   ],
   "source": [
    "# Calcula métricas con umbral ajustable\n",
    "for umbral in [0.5, 0.3, 0.2, 0.1]:\n",
    "    predicciones = (modelo.predict(X_val) > umbral).astype(int).flatten()\n",
    "    accuracy = np.mean(predicciones == y_val) * 100\n",
    "    precision = precision_score(y_val, predicciones)\n",
    "    recall = recall_score(y_val, predicciones)\n",
    "    f1 = f1_score(y_val, predicciones)\n",
    "    \n",
    "    print(f\"\\nResultados con umbral {umbral}:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
